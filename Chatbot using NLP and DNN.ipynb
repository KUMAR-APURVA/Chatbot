{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction:\n",
    "\n",
    "In this project, we have tried to implement a **CHATBOT** using **Natural Language Processing (NLP)** and **Neural Networks**. \n",
    "\n",
    "A **chatbot** is a software application used to conduct an online chat conversation via text or text-to-speech, in lieu of providing direct contact with a live human agent. It is designed to convincingly simulate the way a human would behave as a conversational partner. \n",
    "\n",
    "We have created a chatbot for Data Structures and Algorithms. A **User** can ask anything related to Data Structures and Algorithms and the **Bot** will answer it accordingly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries:\n",
    "\n",
    "We have used various Python Modules in this Notebook.\n",
    "* `nltk` refers to Natural Language Toolkit and helps to perform NLP\n",
    "* `tensorflow` is a free and open-source software library for machine learning. It's primary focus is on training and inference of deep neural networks.\n",
    "* `tflearn` is a modular and transparent deep learning library built on top of Tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\KUMAR APURV\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\compat\\v2_compat.py:88: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "curses is not supported on this machine (please install/reinstall curses for an optimal experience)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\KUMAR\n",
      "[nltk_data]     APURV\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import nltk\n",
    "import numpy\n",
    "import random\n",
    "import tensorflow\n",
    "import tflearn\n",
    "import pickle\n",
    "from termcolor import colored\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "nltk.download('punkt')\n",
    "stemmer = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset:\n",
    "\n",
    "We have created **`intents.json`** which is a json file containing our training data that trains our model on Data Structures and Algorithms. It contains a dictionary with **key** as `intents` and **value** as a `list of dictionaries`. In the list, each dictionary have three keys which are:\n",
    "* `tag`: It represents the topic.\n",
    "* `patterns`: It contains various ways of asking the questions related to that topic.\n",
    "* `responses`: It contains the answers to that questions.\n",
    "\n",
    "Here, we have loaded the intents.json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('intents.json') as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `'words'` is a bag of words which contains all the stemmed words. For this purpose, we have used Tokenization and Stemming. \n",
    "\n",
    "* `'labels'` are used for storing all the tags. \n",
    "\n",
    "* `'training'` is a 2D matrix in which each row is a binary representation of a pattern. Each row contains value as **1** where the words of pattern matches with the words in 'bag of words' otherwise **0**.\n",
    "\n",
    "* `'output'` is a 2D matrix used to map each row of training to it's corresponding tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open(\"data.pickle\", \"rb\") as f:\n",
    "        words, labels, training, output = pickle.load(f)\n",
    "except:\n",
    "    words = []\n",
    "    labels = []\n",
    "    docs_x = []\n",
    "    docs_y = []\n",
    "\n",
    "    for intent in data[\"intents\"]:\n",
    "        for pattern in intent[\"patterns\"]:\n",
    "            wrds = nltk.word_tokenize(pattern)\n",
    "            words.extend(wrds)\n",
    "            docs_x.append(wrds)\n",
    "            docs_y.append(intent[\"tag\"])\n",
    "\n",
    "        if intent[\"tag\"] not in labels:\n",
    "            labels.append(intent[\"tag\"])\n",
    "\n",
    "    words = [stemmer.stem(w.lower()) for w in words if w != \"?\"]\n",
    "    words = sorted(list(set(words)))\n",
    "\n",
    "    labels = sorted(labels)\n",
    "\n",
    "    training = []\n",
    "    output = []\n",
    "\n",
    "    out_empty = [0 for _ in range(len(labels))]\n",
    "\n",
    "    for x, doc in enumerate(docs_x):\n",
    "        bag = []\n",
    "\n",
    "        wrds = [stemmer.stem(w.lower()) for w in doc]\n",
    "\n",
    "        for w in words:\n",
    "            if w in wrds:\n",
    "                bag.append(1)\n",
    "            else:\n",
    "                bag.append(0)\n",
    "\n",
    "        output_row = out_empty[:]\n",
    "        output_row[labels.index(docs_y[x])] = 1\n",
    "\n",
    "        training.append(bag)\n",
    "        output.append(output_row)\n",
    "\n",
    "\n",
    "    training = numpy.array(training)\n",
    "    output = numpy.array(output)\n",
    "\n",
    "    with open(\"data.pickle\", \"wb\") as f:\n",
    "        pickle.dump((words, labels, training, output), f)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we have created a **Deep Neural Network**. It contains an `input layer`, `5 hidden layers` each containing 100 features and an `output layer`. It is a fully connected network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\KUMAR APURV\\Anaconda3\\lib\\site-packages\\tflearn\\initializations.py:165: calling TruncatedNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()\n",
    "\n",
    "net = tflearn.input_data(shape=[None, len(training[0])])\n",
    "net = tflearn.fully_connected(net, 100)\n",
    "net = tflearn.fully_connected(net, 100)\n",
    "net = tflearn.fully_connected(net, 100)\n",
    "net = tflearn.fully_connected(net, 100)\n",
    "net = tflearn.fully_connected(net, 100)\n",
    "net = tflearn.fully_connected(net, len(output[0]), activation=\"softmax\")\n",
    "net = tflearn.regression(net)\n",
    "\n",
    "model = tflearn.DNN(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we are searching if we already have a trained model. If yes, we will load the model, otherwise we will train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:\\Users\\KUMAR APURV\\Desktop\\Chatbot\\model.tflearn\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    model.load('model.tflearn')\n",
    "except:\n",
    "    from tensorflow.python.framework import ops\n",
    "    ops.reset_default_graph()\n",
    "\n",
    "    net = tflearn.input_data(shape=[None, len(training[0])])\n",
    "    net = tflearn.fully_connected(net, 100)\n",
    "    net = tflearn.fully_connected(net, 100)\n",
    "    net = tflearn.fully_connected(net, 100)\n",
    "    net = tflearn.fully_connected(net, 100)\n",
    "    net = tflearn.fully_connected(net, 100)\n",
    "    net = tflearn.fully_connected(net, len(output[0]), activation=\"softmax\")\n",
    "    net = tflearn.regression(net)\n",
    "\n",
    "    model = tflearn.DNN(net)\n",
    "    model.fit(training, output, n_epoch=1000, batch_size=8, show_metric=True)\n",
    "    model.save(\"model.tflearn\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is used to convert the input string (pattern) into it's binary representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(s, words):\n",
    "    bag = [0 for _ in range(len(words))]\n",
    "\n",
    "    s_words = nltk.word_tokenize(s)\n",
    "    s_words = [stemmer.stem(word.lower()) for word in s_words]\n",
    "\n",
    "    for se in s_words:\n",
    "        for i, w in enumerate(words):\n",
    "            if w == se:\n",
    "                bag[i] = 1\n",
    "\n",
    "    return numpy.array(bag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This the function used to call the chatbot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat():\n",
    "    print(\"Start talking with the bot (type quit to stop)!\")\n",
    "    while True:\n",
    "        print()\n",
    "        print(colored(\"You:\",'blue'))\n",
    "        inp = input()\n",
    "        if inp.lower() == \"quit\":\n",
    "            break\n",
    "\n",
    "        results = model.predict([convert(inp, words)])[0]\n",
    "        results_index = numpy.argmax(results)\n",
    "        tag = labels[results_index]\n",
    "        \n",
    "        if (results[results_index] > 0.8):\n",
    "            for tg in data[\"intents\"]:\n",
    "                if tg['tag'] == tag:\n",
    "                    responses = tg['responses']\n",
    "            print(colored(\"Bot: \",'red'))\n",
    "            final_response = random.choice(responses)\n",
    "            print(final_response)\n",
    "            \n",
    "        else:\n",
    "            print(colored(\"Bot: \",'red'))\n",
    "            print(\"I'm not sure about that. Try again.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start talking with the bot (type quit to stop)!\n",
      "\n",
      "\u001b[34mYou:\u001b[0m\n",
      "Hi\n",
      "\u001b[31mBot: \u001b[0m\n",
      "Hello\n",
      "\n",
      "\u001b[34mYou:\u001b[0m\n",
      "Who are you?\n",
      "\u001b[31mBot: \u001b[0m\n",
      "I am a chatbot\n",
      "\n",
      "\u001b[34mYou:\u001b[0m\n",
      "What is Data Structures?\n",
      "\u001b[31mBot: \u001b[0m\n",
      "A data structure is a particular way of organizing data in a computer so that it can be used effectively.\n",
      "\n",
      "\u001b[34mYou:\u001b[0m\n",
      "Tell me about Linked List\n",
      "\u001b[31mBot: \u001b[0m\n",
      "A linked list is a linear data structure, in which the elements are not stored at contiguous memory locations. \n",
      "The elements in a linked list are linked using pointers. \n",
      "There are 3 different implementations of Linked List available, they are\n",
      "1. Singly Linked List\n",
      "2. Doubly Linked List\n",
      "3. Circular Linked List\n",
      "\n",
      "\u001b[34mYou:\u001b[0m\n",
      "Do you know about queues?\n",
      "\u001b[31mBot: \u001b[0m\n",
      "A Queue is a linear structure which follows a particular order in which the operations are performed. \n",
      "The order is First In First Out (FIFO). \n",
      "A good example of a queue is any queue of consumers for a resource where the consumer that came first is served first. \n",
      "The difference between stacks and queues is in removing. In a stack we remove the item the most recently added; \n",
      "in a queue, we remove the item the least recently added.\n",
      "\n",
      "\u001b[34mYou:\u001b[0m\n",
      "Tell me a joke\n",
      "\u001b[31mBot: \u001b[0m\n",
      "Why did the hipster burn his mouth? He drank the coffee before it was cool.\n",
      "\n",
      "\u001b[34mYou:\u001b[0m\n",
      "Do you know me?\n",
      "\u001b[31mBot: \u001b[0m\n",
      "I'm not sure about that. Try again.\n",
      "\n",
      "\u001b[34mYou:\u001b[0m\n",
      "Goodbye\n",
      "\u001b[31mBot: \u001b[0m\n",
      "Bye! Come back again soon.\n",
      "\n",
      "\u001b[34mYou:\u001b[0m\n",
      "quit\n"
     ]
    }
   ],
   "source": [
    "chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
